,rating_int,confidence_int,review,forum,abstract,id_x,conf_year,review_tokens,review_num_tokens,id_y,tcdate,tmdate,number,replyto,title,decision,comment,conf_name,confidence
0,6,4.0,"there is a lot of recent work on link-prediction in knowledge graphs. one approach is based on embedding entities and relations in a knowledge graph into vector spaces, and the other is based on finding rules that imply relations, and then using these rules to find new links or facts. this paper takes the latter approach. within the area of rule-based methods, a number of recent papers have used neural network methods to simultaneously generate rules and to find rule-weights or other related parameters (indicating how important individual rules are). simultaneously solving for rules and rule-weights is a difficult task. in this paper, the authors propose a method where they separate the rule generation process from the weight/parameter calculation process. more importantly, they add a feedback loop from the weight calculation routine (""reasoning predictor"") to the rule generation routine (""rule generator""), which in my opinion is novel, even though there have been a few recent attempts (xiong et. al. 2017) to use reinforcement learning to search for rules. the rule generator in this paper uses a recurrent neural network (rnn) and the parameters of this rnn are modified by the reasoning predictor. in other words, the iterative process has the feature that new rule generation is influenced by the calculated weights of previously generated rules. the authors perform numerical experiments on 4 standard knowledge graphs to demonstrate the performance of their method

strong points

1. clean probabilistic framework to jointly solve for rule generation and rule weight calculation via an iterative process where rule weight calculation (of existing rules) influences subsequent rule generation.

2. good results

3. the authors take into account recent concerns on the quality of knowledge graph link prediction methods voiced in sun et. al. (2020). many papers use a ranking approach that yields excessively good scores to low-quality results (if many entities including the correct one receive equal scores as potential solutions to the query (h,r,?) during testing , where h is an entity and r is a relation, then the correct entity is given a high score), and the authors fix one of the issues in such ranking approaches.

weak points

1. in many rule based methods (such as neural lp), the goal is to find a set of (weighted) rules that imply a single relation.
in this paper, it seems that the authors generate a set of weighted rules for each ""query"" of the form (h,r, ?) derived from a fact/triplet of the form (h,r,t) where h,t are entities and r is a relation. my issues with this are: 

 a) computation time, given that the number of relations maybe in the few hundreds (e.g. fb15k-237), whereas the number of facts/triplets is significantly larger. the authors do not say anything about computation time (this seems standard in the area).

 b) in the test-phase, it is not clear how one deals with entities which were not present in the training set? if one had a rule for a relation, then one would still be  able to use the rule.

3. though the authors fix some issues with over-optimistic ranking approaches during testing, they do not clarify what happens when many entities receive equal non-zero scores. in other words, they do not explain what rank they give to the correct entity for the query (h,r,?) if it receives an equal probability score to a number of other entities. 

4. the authors focus on the theoretical aspects of the paper. however, replicating the experiments in this paper seems to be difficult. the high-level ideas are easy to follow, but the precise way the algorithm is put together is hard to follow.
    
recommendation

i recommend accepting this paper.
my acceptance recommendation is based on the ""strong points"" 1 and 3. the paper gives a well-founded approach to improving rule-generation based on previously calculated rule weights, and uses an improved scoring method, so that high scores are more meaningful than in some prior papers.

questions

1. is my understanding correct that you build rules for each query q = (h,r,?) and not for each relation? this seems to be implied in figure 2. please explain how you will deal with head/tail entities in the test set that are missing from the training set?

2. you use ""reverse arcs"" in training set, but you do not make clear if you use inverse relations in the rules you generate. do you?

3. in the sun et. al. paper, they talk about doing a ""fair comparison"" when an algorithm gives a certain probability/score to the correct answer (and to many incorrect answers) of a query. you only use their suggested fix for the case correct answers are given zero probabilities. what happens when multiplies entities are given identical nonzero probabilities?

4. can you clarify what rnnlogic with embedding is? i am guessing it is related to assigning path scores via embedding? please make this explicit.

5. will your method be computationally more expensive than methods which generate rules for individual relations?

6. in trying to answer a query (h,r,?), it is clear you may use rules that include the relation 'r'. what is to stop you from using rules of the from 'r(x,y) and s(y,z) and s^-1(z,y)'  and other similar rules which implicitly create trivial relational paths? 

7. there are a number of papers in the area of rule-learning for simple binary classification tasks where rule weight calculation influences subsequent rule generation. see papers on boosting classifiers, especially boosting rule-based classifiers e.g. eckstein and goldberg (icml 2010). you should probably give a reference to such work. 

8. in the definition of the probability value in equation (5), you are implicitly assuming that if a rule creates multiple paths from a head entity h to multiple entities other than e (say b, and c), than the rule is not good for e. but what if (h,r,b) and (h,r,c) are facts in the training set in addition to (h,r,e)? should you be penalizing this rule (or set of rules z)?

9. it seems to me that you are implicitly assuming that if a fact (h,r,e) is missing from the training set, then it is not true; in other words, a closed-world assumption. is this correct? if so, it would help to mention it explicitly.

though i like the paper, and i believe it has good results, i am now seriously concerned about the quality of the numbers in tables 1 and 2. the origin of many of the results in these tables is in doubt. the authors say that the results for transe, rotate, conve, complex , distmult, and minerva are taken from the corresponding papers. but when i look at these papers, i see different (or no) numbers.

the values for minerva in the current paper match for fb15k-237, but do not match for wn18rr. the values in the minerva paper for umls or kinship are way better than in the current paper. now minerva uses a different evaluation protocol, but i seriously doubt the minerva numbers for umls and kinship.

rotate has no numbers for umls or kinship.

complex has none of the numbers reported here.

distmult has none of the numbers reported here.

the numbers in the current paper for transe, distmult, rotate, conve and complex seem to be taken from the rotate paper. but rotate has no numbers for umls or kinship.

so overall, the provenance of the numbers in table 1 and 2 is in serious doubt. i realize that the authors do not have a chance to respond and modify the paper. i hope the pc members can weigh in on what can be done at this stage. even though i like this paper, my score will go down based on the poor quality of tables 1 and 2.",tGZu6DlbreV,"this paper studies learning logic rules for reasoning on knowledge graphs. logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). to address these limitations, this paper proposes a probabilistic model called rnnlogic. rnnlogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. we develop an em-based algorithm for optimization. in each iteration, the reasoning predictor is updated to explore some generated logic rules for reasoning. then in the e-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the m-step, the rule generator is updated with the rules selected in the e-step. experiments on four datasets prove the effectiveness of rnnlogic.",tGZu6DlbreV,2021,"['there', 'is', 'a', 'lot', 'of', 'recent', 'work', 'on', 'link-prediction', 'in', 'knowledge', 'graphs', '.', 'one', 'approach', 'is', 'based', 'on', 'embedding', 'entities', 'and', 'relations', 'in', 'a', 'knowledge', 'graph', 'into', 'vector', 'spaces', ',', 'and', 'the', 'other', 'is', 'based', 'on', 'finding', 'rules', 'that', 'imply', 'relations', ',', 'and', 'then', 'using', 'these', 'rules', 'to', 'find', 'new', 'links', 'or', 'facts', '.', 'this', 'paper', 'takes', 'the', 'latter', 'approach', '.', 'within', 'the', 'area', 'of', 'rule-based', 'methods', ',', 'a', 'number', 'of', 'recent', 'papers', 'have', 'used', 'neural', 'network', 'methods', 'to', 'simultaneously', 'generate', 'rules', 'and', 'to', 'find', 'rule-weights', 'or', 'other', 'related', 'parameters', '(', 'indicating', 'how', 'important', 'individual', 'rules', 'are', ')', '.', 'simultaneously', 'solving', 'for', 'rules', 'and', 'rule-weights', 'is', 'a', 'difficult', 'task', '.', 'in', 'this', 'paper', ',', 'the', 'authors', 'propose', 'a', 'method', 'where', 'they', 'separate', 'the', 'rule', 'generation', 'process', 'from', 'the', 'weight/parameter', 'calculation', 'process', '.', 'more', 'importantly', ',', 'they', 'add', 'a', 'feedback', 'loop', 'from', 'the', 'weight', 'calculation', 'routine', '(', '``', 'reasoning', 'predictor', ""''"", ')', 'to', 'the', 'rule', 'generation', 'routine', '(', '``', 'rule', 'generator', ""''"", ')', ',', 'which', 'in', 'my', 'opinion', 'is', 'novel', ',', 'even', 'though', 'there', 'have', 'been', 'a', 'few', 'recent', 'attempts', '(', 'xiong', 'et', '.', 'al', '.', '2017', ')', 'to', 'use', 'reinforcement', 'learning', 'to', 'search', 'for', 'rules', '.', 'the', 'rule', 'generator', 'in', 'this', 'paper', 'uses', 'a', 'recurrent', 'neural', 'network', '(', 'rnn', ')', 'and', 'the', 'parameters', 'of', 'this', 'rnn', 'are', 'modified', 'by', 'the', 'reasoning', 'predictor', '.', 'in', 'other', 'words', ',', 'the', 'iterative', 'process', 'has', 'the', 'feature', 'that', 'new', 'rule', 'generation', 'is', 'influenced', 'by', 'the', 'calculated', 'weights', 'of', 'previously', 'generated', 'rules', '.', 'the', 'authors', 'perform', 'numerical', 'experiments', 'on', '4', 'standard', 'knowledge', 'graphs', 'to', 'demonstrate', 'the', 'performance', 'of', 'their', 'method', 'strong', 'points', '1.', 'clean', 'probabilistic', 'framework', 'to', 'jointly', 'solve', 'for', 'rule', 'generation', 'and', 'rule', 'weight', 'calculation', 'via', 'an', 'iterative', 'process', 'where', 'rule', 'weight', 'calculation', '(', 'of', 'existing', 'rules', ')', 'influences', 'subsequent', 'rule', 'generation', '.', '2.', 'good', 'results', '3.', 'the', 'authors', 'take', 'into', 'account', 'recent', 'concerns', 'on', 'the', 'quality', 'of', 'knowledge', 'graph', 'link', 'prediction', 'methods', 'voiced', 'in', 'sun', 'et', '.', 'al', '.', '(', '2020', ')', '.', 'many', 'papers', 'use', 'a', 'ranking', 'approach', 'that', 'yields', 'excessively', 'good', 'scores', 'to', 'low-quality', 'results', '(', 'if', 'many', 'entities', 'including', 'the', 'correct', 'one', 'receive', 'equal', 'scores', 'as', 'potential', 'solutions', 'to', 'the', 'query', '(', 'h', ',', 'r', ',', '?', ')', 'during', 'testing', ',', 'where', 'h', 'is', 'an', 'entity', 'and', 'r', 'is', 'a', 'relation', ',', 'then', 'the', 'correct', 'entity', 'is', 'given', 'a', 'high', 'score', ')', ',', 'and', 'the', 'authors', 'fix', 'one', 'of', 'the', 'issues', 'in', 'such', 'ranking', 'approaches', '.', 'weak', 'points', '1.', 'in', 'many', 'rule', 'based', 'methods', '(', 'such', 'as', 'neural', 'lp', ')', ',', 'the', 'goal', 'is', 'to', 'find', 'a', 'set', 'of', '(', 'weighted', ')', 'rules', 'that', 'imply', 'a', 'single', 'relation', '.', 'in', 'this', 'paper', ',', 'it', 'seems', 'that', 'the', 'authors', 'generate', 'a', 'set', 'of', 'weighted', 'rules', 'for', 'each', '``', 'query', ""''"", 'of', 'the', 'form', '(', 'h', ',', 'r', ',', '?', ')', 'derived', 'from', 'a', 'fact/triplet', 'of', 'the', 'form', '(', 'h', ',', 'r', ',', 't', ')', 'where', 'h', ',', 't', 'are', 'entities', 'and', 'r', 'is', 'a', 'relation', '.', 'my', 'issues', 'with', 'this', 'are', ':', 'a', ')', 'computation', 'time', ',', 'given', 'that', 'the', 'number', 'of', 'relations', 'maybe', 'in', 'the', 'few', 'hundreds', '(', 'e.g', '.', 'fb15k-237', ')', ',', 'whereas', 'the', 'number', 'of', 'facts/triplets', 'is', 'significantly', 'larger', '.', 'the', 'authors', 'do', 'not', 'say', 'anything', 'about', 'computation', 'time', '(', 'this', 'seems', 'standard', 'in', 'the', 'area', ')', '.', 'b', ')', 'in', 'the', 'test-phase', ',', 'it', 'is', 'not', 'clear', 'how', 'one', 'deals', 'with', 'entities', 'which', 'were', 'not', 'present', 'in', 'the', 'training', 'set', '?', 'if', 'one', 'had', 'a', 'rule', 'for', 'a', 'relation', ',', 'then', 'one', 'would', 'still', 'be', 'able', 'to', 'use', 'the', 'rule', '.', '3.', 'though', 'the', 'authors', 'fix', 'some', 'issues', 'with', 'over-optimistic', 'ranking', 'approaches', 'during', 'testing', ',', 'they', 'do', 'not', 'clarify', 'what', 'happens', 'when', 'many', 'entities', 'receive', 'equal', 'non-zero', 'scores', '.', 'in', 'other', 'words', ',', 'they', 'do', 'not', 'explain', 'what', 'rank', 'they', 'give', 'to', 'the', 'correct', 'entity', 'for', 'the', 'query', '(', 'h', ',', 'r', ',', '?', ')', 'if', 'it', 'receives', 'an', 'equal', 'probability', 'score', 'to', 'a', 'number', 'of', 'other', 'entities', '.', '4.', 'the', 'authors', 'focus', 'on', 'the', 'theoretical', 'aspects', 'of', 'the', 'paper', '.', 'however', ',', 'replicating', 'the', 'experiments', 'in', 'this', 'paper', 'seems', 'to', 'be', 'difficult', '.', 'the', 'high-level', 'ideas', 'are', 'easy', 'to', 'follow', ',', 'but', 'the', 'precise', 'way', 'the', 'algorithm', 'is', 'put', 'together', 'is', 'hard', 'to', 'follow', '.', 'recommendation', 'i', 'recommend', 'accepting', 'this', 'paper', '.', 'my', 'acceptance', 'recommendation', 'is', 'based', 'on', 'the', '``', 'strong', 'points', ""''"", '1', 'and', '3.', 'the', 'paper', 'gives', 'a', 'well-founded', 'approach', 'to', 'improving', 'rule-generation', 'based', 'on', 'previously', 'calculated', 'rule', 'weights', ',', 'and', 'uses', 'an', 'improved', 'scoring', 'method', ',', 'so', 'that', 'high', 'scores', 'are', 'more', 'meaningful', 'than', 'in', 'some', 'prior', 'papers', '.', 'questions', '1.', 'is', 'my', 'understanding', 'correct', 'that', 'you', 'build', 'rules', 'for', 'each', 'query', 'q', '=', '(', 'h', ',', 'r', ',', '?', ')', 'and', 'not', 'for', 'each', 'relation', '?', 'this', 'seems', 'to', 'be', 'implied', 'in', 'figure', '2.', 'please', 'explain', 'how', 'you', 'will', 'deal', 'with', 'head/tail', 'entities', 'in', 'the', 'test', 'set', 'that', 'are', 'missing', 'from', 'the', 'training', 'set', '?', '2.', 'you', 'use', '``', 'reverse', 'arcs', ""''"", 'in', 'training', 'set', ',', 'but', 'you', 'do', 'not', 'make', 'clear', 'if', 'you', 'use', 'inverse', 'relations', 'in', 'the', 'rules', 'you', 'generate', '.', 'do', 'you', '?', '3.', 'in', 'the', 'sun', 'et', '.', 'al', '.', 'paper', ',', 'they', 'talk', 'about', 'doing', 'a', '``', 'fair', 'comparison', ""''"", 'when', 'an', 'algorithm', 'gives', 'a', 'certain', 'probability/score', 'to', 'the', 'correct', 'answer', '(', 'and', 'to', 'many', 'incorrect', 'answers', ')', 'of', 'a', 'query', '.', 'you', 'only', 'use', 'their', 'suggested', 'fix', 'for', 'the', 'case', 'correct', 'answers', 'are', 'given', 'zero', 'probabilities', '.', 'what', 'happens', 'when', 'multiplies', 'entities', 'are', 'given', 'identical', 'nonzero', 'probabilities', '?', '4.', 'can', 'you', 'clarify', 'what', 'rnnlogic', 'with', 'embedding', 'is', '?', 'i', 'am', 'guessing', 'it', 'is', 'related', 'to', 'assigning', 'path', 'scores', 'via', 'embedding', '?', 'please', 'make', 'this', 'explicit', '.', '5.', 'will', 'your', 'method', 'be', 'computationally', 'more', 'expensive', 'than', 'methods', 'which', 'generate', 'rules', 'for', 'individual', 'relations', '?', '6.', 'in', 'trying', 'to', 'answer', 'a', 'query', '(', 'h', ',', 'r', ',', '?', ')', ',', 'it', 'is', 'clear', 'you', 'may', 'use', 'rules', 'that', 'include', 'the', 'relation', ""'"", 'r', ""'"", '.', 'what', 'is', 'to', 'stop', 'you', 'from', 'using', 'rules', 'of', 'the', 'from', ""'"", 'r', '(', 'x', ',', 'y', ')', 'and', 's', '(', 'y', ',', 'z', ')', 'and', 's^-1', '(', 'z', ',', 'y', ')', ""'"", 'and', 'other', 'similar', 'rules', 'which', 'implicitly', 'create', 'trivial', 'relational', 'paths', '?', '7.', 'there', 'are', 'a', 'number', 'of', 'papers', 'in', 'the', 'area', 'of', 'rule-learning', 'for', 'simple', 'binary', 'classification', 'tasks', 'where', 'rule', 'weight', 'calculation', 'influences', 'subsequent', 'rule', 'generation', '.', 'see', 'papers', 'on', 'boosting', 'classifiers', ',', 'especially', 'boosting', 'rule-based', 'classifiers', 'e.g', '.', 'eckstein', 'and', 'goldberg', '(', 'icml', '2010', ')', '.', 'you', 'should', 'probably', 'give', 'a', 'reference', 'to', 'such', 'work', '.', '8.', 'in', 'the', 'definition', 'of', 'the', 'probability', 'value', 'in', 'equation', '(', '5', ')', ',', 'you', 'are', 'implicitly', 'assuming', 'that', 'if', 'a', 'rule', 'creates', 'multiple', 'paths', 'from', 'a', 'head', 'entity', 'h', 'to', 'multiple', 'entities', 'other', 'than', 'e', '(', 'say', 'b', ',', 'and', 'c', ')', ',', 'than', 'the', 'rule', 'is', 'not', 'good', 'for', 'e.', 'but', 'what', 'if', '(', 'h', ',', 'r', ',', 'b', ')', 'and', '(', 'h', ',', 'r', ',', 'c', ')', 'are', 'facts', 'in', 'the', 'training', 'set', 'in', 'addition', 'to', '(', 'h', ',', 'r', ',', 'e', ')', '?', 'should', 'you', 'be', 'penalizing', 'this', 'rule', '(', 'or', 'set', 'of', 'rules', 'z', ')', '?', '9.', 'it', 'seems', 'to', 'me', 'that', 'you', 'are', 'implicitly', 'assuming', 'that', 'if', 'a', 'fact', '(', 'h', ',', 'r', ',', 'e', ')', 'is', 'missing', 'from', 'the', 'training', 'set', ',', 'then', 'it', 'is', 'not', 'true', ';', 'in', 'other', 'words', ',', 'a', 'closed-world', 'assumption', '.', 'is', 'this', 'correct', '?', 'if', 'so', ',', 'it', 'would', 'help', 'to', 'mention', 'it', 'explicitly', '.', 'though', 'i', 'like', 'the', 'paper', ',', 'and', 'i', 'believe', 'it', 'has', 'good', 'results', ',', 'i', 'am', 'now', 'seriously', 'concerned', 'about', 'the', 'quality', 'of', 'the', 'numbers', 'in', 'tables', '1', 'and', '2.', 'the', 'origin', 'of', 'many', 'of', 'the', 'results', 'in', 'these', 'tables', 'is', 'in', 'doubt', '.', 'the', 'authors', 'say', 'that', 'the', 'results', 'for', 'transe', ',', 'rotate', ',', 'conve', ',', 'complex', ',', 'distmult', ',', 'and', 'minerva', 'are', 'taken', 'from', 'the', 'corresponding', 'papers', '.', 'but', 'when', 'i', 'look', 'at', 'these', 'papers', ',', 'i', 'see', 'different', '(', 'or', 'no', ')', 'numbers', '.', 'the', 'values', 'for', 'minerva', 'in', 'the', 'current', 'paper', 'match', 'for', 'fb15k-237', ',', 'but', 'do', 'not', 'match', 'for', 'wn18rr', '.', 'the', 'values', 'in', 'the', 'minerva', 'paper', 'for', 'umls', 'or', 'kinship', 'are', 'way', 'better', 'than', 'in', 'the', 'current', 'paper', '.', 'now', 'minerva', 'uses', 'a', 'different', 'evaluation', 'protocol', ',', 'but', 'i', 'seriously', 'doubt', 'the', 'minerva', 'numbers', 'for', 'umls', 'and', 'kinship', '.', 'rotate', 'has', 'no', 'numbers', 'for', 'umls', 'or', 'kinship', '.', 'complex', 'has', 'none', 'of', 'the', 'numbers', 'reported', 'here', '.', 'distmult', 'has', 'none', 'of', 'the', 'numbers', 'reported', 'here', '.', 'the', 'numbers', 'in', 'the', 'current', 'paper', 'for', 'transe', ',', 'distmult', ',', 'rotate', ',', 'conve', 'and', 'complex', 'seem', 'to', 'be', 'taken', 'from', 'the', 'rotate', 'paper', '.', 'but', 'rotate', 'has', 'no', 'numbers', 'for', 'umls', 'or', 'kinship', '.', 'so', 'overall', ',', 'the', 'provenance', 'of', 'the', 'numbers', 'in', 'table', '1', 'and', '2', 'is', 'in', 'serious', 'doubt', '.', 'i', 'realize', 'that', 'the', 'authors', 'do', 'not', 'have', 'a', 'chance', 'to', 'respond', 'and', 'modify', 'the', 'paper', '.', 'i', 'hope', 'the', 'pc', 'members', 'can', 'weigh', 'in', 'on', 'what', 'can', 'be', 'done', 'at', 'this', 'stage', '.', 'even', 'though', 'i', 'like', 'this', 'paper', ',', 'my', 'score', 'will', 'go', 'down', 'based', 'on', 'the', 'poor', 'quality', 'of', 'tables', '1', 'and', '2', '.']",1537,uVb3Ow-12JH,1610045694102,1610474017438,1,tGZu6DlbreV,Final Decision,Accept (Poster),"There is a consensus among the reviewers that the work is interesting and the paper should be accepted.  Nevertheless, several reviewers struggled with understanding the details. While the authors  (largely successfully) addressed these concerns, I believe that the paper is still too dense and hard to follow, I would encourage the authors to invest more time into improving its readability.  One important point which came late in the discussion is the provenance of baseline scores in the result tables (see the review by AnonReviewer3, the current manuscript claims that the numbers are taken from the original papers while in some cases, the numbers cannot be located in these papers). Unfortunately, the authors did not have a chance to respond to this criticism, and fortunately we could trace the key numbers and establish that the results are strong enough to warrant accepting the submission. Still, we would ask the reviewers to fix this issue in the final version.",ICLR2021,
1,7,4.0,"in this paper, the author proposes rnnlogic for learning fol rules from the knowledge graph. the proposed method assigns embeddings for each relation type and uses rnn module to generate chain-like rule candidates. candidates are evaluated with a separate evaluation module that computes the scores. the rule scores are then taken to update the generator module using em.

i think the idea of separating rule generation and evaluation is interesting, and using em to jointly train the modules is also novel. however, i think the current paper writing is convoluted and prone to notations issues. one needs to constantly refer to appendix to understand the proposed method. for example:

- why is eq4 a valid answer score? i think it's related to the commonly-used graph traversal score, but the author should clarify this in the main text.

- psi_w(rule) is not defined in the main text. and i found its definition in c1 to be problematic too: it's defined over score_w(t|rule) - but isn't that the score_w is defined with psi_w(rule) and psi_w(path) already in eq4?

- i suggest the author to elaborate more why rotate can be used to generate psi_w(path), as it's not very intuitive to me why this would help

some claims are not well addressed:

- in section 2, the author criticizes the current differentiable ilp methods to lack ways ""to determine the importance of rules with the learned weights due to the high dimensionality."". however, many methods such as diff-ilp and neurallp can indeed learn the rule weights.

- backward-chaining methods such as neurallp are indeed efficient for problems with high dimensionality as well.

- there seems to be no justification or experiments to demonstrate how the proposed method can do better than the previous ones on these properties.

model details:

- the author claims the psi_w(path) can be fixed to 1, in this case, what's been learned w.r.t parameter w? is psi_w(rule) a learnable module as well?

model efficiency:

- evaluating eq 4 and eq 5 requires to sum over all possible paths and rules. this process can be efficient for methods utilizing matrix multiplication such as neurallp. but it's unclear to me if a similar method is used in rnnlogic. if not, how well does the model scale to long paths and kbs with large grounding space?

- also, the em algorithm requires to hard sample top-k rules (k=1000) for each data instance at each iteration. i'm concerned about the efficiency of this method. it would be helpful if the author can provide the runtime comparison in the experiment as well.


experiment:

- many baseline scores on the fb15k and wn18 are cited from the original paper. but why do neurallp and drum get rerun in these twos benchmark?

- in appendix d, the author mentions that the entity embeddings are in fact *pre-trained* using rotate. i find this setting to be unjustified and can lead to unfair comparisons against both ilp and non-ilp methods.

- in the w/o emb. mode, i assume no embeddings are learned in the reasoning module, so the score should be comparable to the vanilla score used in neurallp. given that the rnnlogic also generates only chain-like rules using rnn, it seems that the proposed method should have searched similar rules and got similar rule scores as the neurallp. can author provide intuitions or insights why rnnlogic significantly outperforms the neurallp even in w/o emb. mode?

overall, i think this work indeed has some novelties but it is currently prone to unjustified claims and confusing writing. the experiment is incomplete and the usage of pre-trained embedding for sota model is unjustified. with that being said, i would recommend weak rejection at this point, but i'm happy to raise the scores if these concerns are addressed.",tGZu6DlbreV,"this paper studies learning logic rules for reasoning on knowledge graphs. logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). to address these limitations, this paper proposes a probabilistic model called rnnlogic. rnnlogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. we develop an em-based algorithm for optimization. in each iteration, the reasoning predictor is updated to explore some generated logic rules for reasoning. then in the e-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the m-step, the rule generator is updated with the rules selected in the e-step. experiments on four datasets prove the effectiveness of rnnlogic.",tGZu6DlbreV,2021,"['in', 'this', 'paper', ',', 'the', 'author', 'proposes', 'rnnlogic', 'for', 'learning', 'fol', 'rules', 'from', 'the', 'knowledge', 'graph', '.', 'the', 'proposed', 'method', 'assigns', 'embeddings', 'for', 'each', 'relation', 'type', 'and', 'uses', 'rnn', 'module', 'to', 'generate', 'chain-like', 'rule', 'candidates', '.', 'candidates', 'are', 'evaluated', 'with', 'a', 'separate', 'evaluation', 'module', 'that', 'computes', 'the', 'scores', '.', 'the', 'rule', 'scores', 'are', 'then', 'taken', 'to', 'update', 'the', 'generator', 'module', 'using', 'em', '.', 'i', 'think', 'the', 'idea', 'of', 'separating', 'rule', 'generation', 'and', 'evaluation', 'is', 'interesting', ',', 'and', 'using', 'em', 'to', 'jointly', 'train', 'the', 'modules', 'is', 'also', 'novel', '.', 'however', ',', 'i', 'think', 'the', 'current', 'paper', 'writing', 'is', 'convoluted', 'and', 'prone', 'to', 'notations', 'issues', '.', 'one', 'needs', 'to', 'constantly', 'refer', 'to', 'appendix', 'to', 'understand', 'the', 'proposed', 'method', '.', 'for', 'example', ':', '-', 'why', 'is', 'eq4', 'a', 'valid', 'answer', 'score', '?', 'i', 'think', 'it', ""'s"", 'related', 'to', 'the', 'commonly-used', 'graph', 'traversal', 'score', ',', 'but', 'the', 'author', 'should', 'clarify', 'this', 'in', 'the', 'main', 'text', '.', '-', 'psi_w', '(', 'rule', ')', 'is', 'not', 'defined', 'in', 'the', 'main', 'text', '.', 'and', 'i', 'found', 'its', 'definition', 'in', 'c1', 'to', 'be', 'problematic', 'too', ':', 'it', ""'s"", 'defined', 'over', 'score_w', '(', 't|rule', ')', '-', 'but', 'is', ""n't"", 'that', 'the', 'score_w', 'is', 'defined', 'with', 'psi_w', '(', 'rule', ')', 'and', 'psi_w', '(', 'path', ')', 'already', 'in', 'eq4', '?', '-', 'i', 'suggest', 'the', 'author', 'to', 'elaborate', 'more', 'why', 'rotate', 'can', 'be', 'used', 'to', 'generate', 'psi_w', '(', 'path', ')', ',', 'as', 'it', ""'s"", 'not', 'very', 'intuitive', 'to', 'me', 'why', 'this', 'would', 'help', 'some', 'claims', 'are', 'not', 'well', 'addressed', ':', '-', 'in', 'section', '2', ',', 'the', 'author', 'criticizes', 'the', 'current', 'differentiable', 'ilp', 'methods', 'to', 'lack', 'ways', '``', 'to', 'determine', 'the', 'importance', 'of', 'rules', 'with', 'the', 'learned', 'weights', 'due', 'to', 'the', 'high', 'dimensionality.', ""''"", '.', 'however', ',', 'many', 'methods', 'such', 'as', 'diff-ilp', 'and', 'neurallp', 'can', 'indeed', 'learn', 'the', 'rule', 'weights', '.', '-', 'backward-chaining', 'methods', 'such', 'as', 'neurallp', 'are', 'indeed', 'efficient', 'for', 'problems', 'with', 'high', 'dimensionality', 'as', 'well', '.', '-', 'there', 'seems', 'to', 'be', 'no', 'justification', 'or', 'experiments', 'to', 'demonstrate', 'how', 'the', 'proposed', 'method', 'can', 'do', 'better', 'than', 'the', 'previous', 'ones', 'on', 'these', 'properties', '.', 'model', 'details', ':', '-', 'the', 'author', 'claims', 'the', 'psi_w', '(', 'path', ')', 'can', 'be', 'fixed', 'to', '1', ',', 'in', 'this', 'case', ',', 'what', ""'s"", 'been', 'learned', 'w.r.t', 'parameter', 'w', '?', 'is', 'psi_w', '(', 'rule', ')', 'a', 'learnable', 'module', 'as', 'well', '?', 'model', 'efficiency', ':', '-', 'evaluating', 'eq', '4', 'and', 'eq', '5', 'requires', 'to', 'sum', 'over', 'all', 'possible', 'paths', 'and', 'rules', '.', 'this', 'process', 'can', 'be', 'efficient', 'for', 'methods', 'utilizing', 'matrix', 'multiplication', 'such', 'as', 'neurallp', '.', 'but', 'it', ""'s"", 'unclear', 'to', 'me', 'if', 'a', 'similar', 'method', 'is', 'used', 'in', 'rnnlogic', '.', 'if', 'not', ',', 'how', 'well', 'does', 'the', 'model', 'scale', 'to', 'long', 'paths', 'and', 'kbs', 'with', 'large', 'grounding', 'space', '?', '-', 'also', ',', 'the', 'em', 'algorithm', 'requires', 'to', 'hard', 'sample', 'top-k', 'rules', '(', 'k=1000', ')', 'for', 'each', 'data', 'instance', 'at', 'each', 'iteration', '.', 'i', ""'m"", 'concerned', 'about', 'the', 'efficiency', 'of', 'this', 'method', '.', 'it', 'would', 'be', 'helpful', 'if', 'the', 'author', 'can', 'provide', 'the', 'runtime', 'comparison', 'in', 'the', 'experiment', 'as', 'well', '.', 'experiment', ':', '-', 'many', 'baseline', 'scores', 'on', 'the', 'fb15k', 'and', 'wn18', 'are', 'cited', 'from', 'the', 'original', 'paper', '.', 'but', 'why', 'do', 'neurallp', 'and', 'drum', 'get', 'rerun', 'in', 'these', 'twos', 'benchmark', '?', '-', 'in', 'appendix', 'd', ',', 'the', 'author', 'mentions', 'that', 'the', 'entity', 'embeddings', 'are', 'in', 'fact', '*', 'pre-trained', '*', 'using', 'rotate', '.', 'i', 'find', 'this', 'setting', 'to', 'be', 'unjustified', 'and', 'can', 'lead', 'to', 'unfair', 'comparisons', 'against', 'both', 'ilp', 'and', 'non-ilp', 'methods', '.', '-', 'in', 'the', 'w/o', 'emb', '.', 'mode', ',', 'i', 'assume', 'no', 'embeddings', 'are', 'learned', 'in', 'the', 'reasoning', 'module', ',', 'so', 'the', 'score', 'should', 'be', 'comparable', 'to', 'the', 'vanilla', 'score', 'used', 'in', 'neurallp', '.', 'given', 'that', 'the', 'rnnlogic', 'also', 'generates', 'only', 'chain-like', 'rules', 'using', 'rnn', ',', 'it', 'seems', 'that', 'the', 'proposed', 'method', 'should', 'have', 'searched', 'similar', 'rules', 'and', 'got', 'similar', 'rule', 'scores', 'as', 'the', 'neurallp', '.', 'can', 'author', 'provide', 'intuitions', 'or', 'insights', 'why', 'rnnlogic', 'significantly', 'outperforms', 'the', 'neurallp', 'even', 'in', 'w/o', 'emb', '.', 'mode', '?', 'overall', ',', 'i', 'think', 'this', 'work', 'indeed', 'has', 'some', 'novelties', 'but', 'it', 'is', 'currently', 'prone', 'to', 'unjustified', 'claims', 'and', 'confusing', 'writing', '.', 'the', 'experiment', 'is', 'incomplete', 'and', 'the', 'usage', 'of', 'pre-trained', 'embedding', 'for', 'sota', 'model', 'is', 'unjustified', '.', 'with', 'that', 'being', 'said', ',', 'i', 'would', 'recommend', 'weak', 'rejection', 'at', 'this', 'point', ',', 'but', 'i', ""'m"", 'happy', 'to', 'raise', 'the', 'scores', 'if', 'these', 'concerns', 'are', 'addressed', '.']",722,uVb3Ow-12JH,1610045694102,1610474017438,1,tGZu6DlbreV,Final Decision,Accept (Poster),"There is a consensus among the reviewers that the work is interesting and the paper should be accepted.  Nevertheless, several reviewers struggled with understanding the details. While the authors  (largely successfully) addressed these concerns, I believe that the paper is still too dense and hard to follow, I would encourage the authors to invest more time into improving its readability.  One important point which came late in the discussion is the provenance of baseline scores in the result tables (see the review by AnonReviewer3, the current manuscript claims that the numbers are taken from the original papers while in some cases, the numbers cannot be located in these papers). Unfortunately, the authors did not have a chance to respond to this criticism, and fortunately we could trace the key numbers and establish that the results are strong enough to warrant accepting the submission. Still, we would ask the reviewers to fix this issue in the final version.",ICLR2021,
2,6,2.0,"this paper focuses on learning logic rules via em-based algorithm. the idea in the paper is that e-step would try to generate rules while m-step would update the parameters. the empirical comparisons are interesting and show that the paper improves on prior work. 

there are several aspects that make it very hard for me to understand the paper's contributions and evaluation. i am listing below and hoping that other reviewers or authors would be able to clarify:

1. the paper suggests that the performance is owing to reduction of search space due to e-m step. it is hard to understand what mln tool is being employed here and why mln-based technique would return a suboptimal answer (the combinatorial solvers may return suboptimal answer due to timeout but that should be clarified in this case). it may perhaps be the case that sampling rules from a good distribution allows us to search only over a small space but that needs to contrasted with weakness of mln-based methods. 

2. the paper uses mean ranking for measuring. i am failing to understanding how is the ranking computed for rules and why such a metric is a good approach.  can authors expand on what exactly is being done: i don't understand ""for each query, we compute probability for each entity"" means? ",tGZu6DlbreV,"this paper studies learning logic rules for reasoning on knowledge graphs. logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). to address these limitations, this paper proposes a probabilistic model called rnnlogic. rnnlogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. we develop an em-based algorithm for optimization. in each iteration, the reasoning predictor is updated to explore some generated logic rules for reasoning. then in the e-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the m-step, the rule generator is updated with the rules selected in the e-step. experiments on four datasets prove the effectiveness of rnnlogic.",tGZu6DlbreV,2021,"['this', 'paper', 'focuses', 'on', 'learning', 'logic', 'rules', 'via', 'em-based', 'algorithm', '.', 'the', 'idea', 'in', 'the', 'paper', 'is', 'that', 'e-step', 'would', 'try', 'to', 'generate', 'rules', 'while', 'm-step', 'would', 'update', 'the', 'parameters', '.', 'the', 'empirical', 'comparisons', 'are', 'interesting', 'and', 'show', 'that', 'the', 'paper', 'improves', 'on', 'prior', 'work', '.', 'there', 'are', 'several', 'aspects', 'that', 'make', 'it', 'very', 'hard', 'for', 'me', 'to', 'understand', 'the', 'paper', ""'s"", 'contributions', 'and', 'evaluation', '.', 'i', 'am', 'listing', 'below', 'and', 'hoping', 'that', 'other', 'reviewers', 'or', 'authors', 'would', 'be', 'able', 'to', 'clarify', ':', '1.', 'the', 'paper', 'suggests', 'that', 'the', 'performance', 'is', 'owing', 'to', 'reduction', 'of', 'search', 'space', 'due', 'to', 'e-m', 'step', '.', 'it', 'is', 'hard', 'to', 'understand', 'what', 'mln', 'tool', 'is', 'being', 'employed', 'here', 'and', 'why', 'mln-based', 'technique', 'would', 'return', 'a', 'suboptimal', 'answer', '(', 'the', 'combinatorial', 'solvers', 'may', 'return', 'suboptimal', 'answer', 'due', 'to', 'timeout', 'but', 'that', 'should', 'be', 'clarified', 'in', 'this', 'case', ')', '.', 'it', 'may', 'perhaps', 'be', 'the', 'case', 'that', 'sampling', 'rules', 'from', 'a', 'good', 'distribution', 'allows', 'us', 'to', 'search', 'only', 'over', 'a', 'small', 'space', 'but', 'that', 'needs', 'to', 'contrasted', 'with', 'weakness', 'of', 'mln-based', 'methods', '.', '2.', 'the', 'paper', 'uses', 'mean', 'ranking', 'for', 'measuring', '.', 'i', 'am', 'failing', 'to', 'understanding', 'how', 'is', 'the', 'ranking', 'computed', 'for', 'rules', 'and', 'why', 'such', 'a', 'metric', 'is', 'a', 'good', 'approach', '.', 'can', 'authors', 'expand', 'on', 'what', 'exactly', 'is', 'being', 'done', ':', 'i', 'do', ""n't"", 'understand', '``', 'for', 'each', 'query', ',', 'we', 'compute', 'probability', 'for', 'each', 'entity', ""''"", 'means', '?']",236,uVb3Ow-12JH,1610045694102,1610474017438,1,tGZu6DlbreV,Final Decision,Accept (Poster),"There is a consensus among the reviewers that the work is interesting and the paper should be accepted.  Nevertheless, several reviewers struggled with understanding the details. While the authors  (largely successfully) addressed these concerns, I believe that the paper is still too dense and hard to follow, I would encourage the authors to invest more time into improving its readability.  One important point which came late in the discussion is the provenance of baseline scores in the result tables (see the review by AnonReviewer3, the current manuscript claims that the numbers are taken from the original papers while in some cases, the numbers cannot be located in these papers). Unfortunately, the authors did not have a chance to respond to this criticism, and fortunately we could trace the key numbers and establish that the results are strong enough to warrant accepting the submission. Still, we would ask the reviewers to fix this issue in the final version.",ICLR2021,
3,8,1.0,"in this work, the authors illustrate an approach for learning logical rules starting from knowledge graphs. learning logic rules is more interesting than simply performing link prediction because rules are human-readable and hence provide explainability.
the approach seems interesting and the tackled problem could interest a wide audience. it does not seem extremely novel, but it seems valid to me.
the paper is well-written and self-contained. moreover, the experimental results show that the proposed approach has competitive performance comparing to other systems (even compared with systems that do not learn rules but perform only link prediction).

for all these reasons i think the paper should be accepted for publication.",tGZu6DlbreV,"this paper studies learning logic rules for reasoning on knowledge graphs. logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). to address these limitations, this paper proposes a probabilistic model called rnnlogic. rnnlogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. we develop an em-based algorithm for optimization. in each iteration, the reasoning predictor is updated to explore some generated logic rules for reasoning. then in the e-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the m-step, the rule generator is updated with the rules selected in the e-step. experiments on four datasets prove the effectiveness of rnnlogic.",tGZu6DlbreV,2021,"['in', 'this', 'work', ',', 'the', 'authors', 'illustrate', 'an', 'approach', 'for', 'learning', 'logical', 'rules', 'starting', 'from', 'knowledge', 'graphs', '.', 'learning', 'logic', 'rules', 'is', 'more', 'interesting', 'than', 'simply', 'performing', 'link', 'prediction', 'because', 'rules', 'are', 'human-readable', 'and', 'hence', 'provide', 'explainability', '.', 'the', 'approach', 'seems', 'interesting', 'and', 'the', 'tackled', 'problem', 'could', 'interest', 'a', 'wide', 'audience', '.', 'it', 'does', 'not', 'seem', 'extremely', 'novel', ',', 'but', 'it', 'seems', 'valid', 'to', 'me', '.', 'the', 'paper', 'is', 'well-written', 'and', 'self-contained', '.', 'moreover', ',', 'the', 'experimental', 'results', 'show', 'that', 'the', 'proposed', 'approach', 'has', 'competitive', 'performance', 'comparing', 'to', 'other', 'systems', '(', 'even', 'compared', 'with', 'systems', 'that', 'do', 'not', 'learn', 'rules', 'but', 'perform', 'only', 'link', 'prediction', ')', '.', 'for', 'all', 'these', 'reasons', 'i', 'think', 'the', 'paper', 'should', 'be', 'accepted', 'for', 'publication', '.']",121,uVb3Ow-12JH,1610045694102,1610474017438,1,tGZu6DlbreV,Final Decision,Accept (Poster),"There is a consensus among the reviewers that the work is interesting and the paper should be accepted.  Nevertheless, several reviewers struggled with understanding the details. While the authors  (largely successfully) addressed these concerns, I believe that the paper is still too dense and hard to follow, I would encourage the authors to invest more time into improving its readability.  One important point which came late in the discussion is the provenance of baseline scores in the result tables (see the review by AnonReviewer3, the current manuscript claims that the numbers are taken from the original papers while in some cases, the numbers cannot be located in these papers). Unfortunately, the authors did not have a chance to respond to this criticism, and fortunately we could trace the key numbers and establish that the results are strong enough to warrant accepting the submission. Still, we would ask the reviewers to fix this issue in the final version.",ICLR2021,
4,6,4.0,"pros:

- the different attention techniques seem to consistently improve object detectors across different models. 
- the ablation studies are important in showing the advantage and impact of each proposed module. 
- please clarify if the student models start from random weights or are initialized after the teacher training. in this regard to what extend is the approach is transferable to students with random weights?

cons:

- the authors only show results when training a network of the same structure with the added modules, in an attempt to boost accuracy. however, an important use of knowledge of knowledge distillation is to train smaller models, and such results are missing from the paper. overall, the results in the paper somewhat justify the ""accurate"" part of the title but not the ""efficient"".

- the importance of high ap is a bit overstated. it is important to demonstrate the different behavior between classification and detection networks, however, the same improvement is observed irrespective of the teacher ap performance. 

- the structure of the paper can be improved. the related work section for example can be moved earlier in the paper to give the bigger picture and the position of this work with respect to the literature. 

- for a more complete comparison yolo and ssd networks should be included.

- in the qualitative analysis observation (ii) states that the proposed methods leads to single box per object compared to the baselines. in this case do the baselines use non-maximum suppression? this is a standard post processing technique that alleviates this problem. no explanation is given on why the proposed method leads to this behavior.

the authors have made clear some of my concerns and made revisions accordingly. thus i am in a position now to recommend this paper, thus i update my initial recommendation from 5 to 6.",uKhGRvM8QNH,"knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. however, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. in this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. experiments show that our methods achieve excellent ap improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. for example, faster rcnn (resnet101 backbone) with our distillation achieves 43.9 ap on coco2017, which is 4.1 higher than the baseline. codes have been released on github.",uKhGRvM8QNH,2021,"['pros', ':', '-', 'the', 'different', 'attention', 'techniques', 'seem', 'to', 'consistently', 'improve', 'object', 'detectors', 'across', 'different', 'models', '.', '-', 'the', 'ablation', 'studies', 'are', 'important', 'in', 'showing', 'the', 'advantage', 'and', 'impact', 'of', 'each', 'proposed', 'module', '.', '-', 'please', 'clarify', 'if', 'the', 'student', 'models', 'start', 'from', 'random', 'weights', 'or', 'are', 'initialized', 'after', 'the', 'teacher', 'training', '.', 'in', 'this', 'regard', 'to', 'what', 'extend', 'is', 'the', 'approach', 'is', 'transferable', 'to', 'students', 'with', 'random', 'weights', '?', 'cons', ':', '-', 'the', 'authors', 'only', 'show', 'results', 'when', 'training', 'a', 'network', 'of', 'the', 'same', 'structure', 'with', 'the', 'added', 'modules', ',', 'in', 'an', 'attempt', 'to', 'boost', 'accuracy', '.', 'however', ',', 'an', 'important', 'use', 'of', 'knowledge', 'of', 'knowledge', 'distillation', 'is', 'to', 'train', 'smaller', 'models', ',', 'and', 'such', 'results', 'are', 'missing', 'from', 'the', 'paper', '.', 'overall', ',', 'the', 'results', 'in', 'the', 'paper', 'somewhat', 'justify', 'the', '``', 'accurate', ""''"", 'part', 'of', 'the', 'title', 'but', 'not', 'the', '``', 'efficient', ""''"", '.', '-', 'the', 'importance', 'of', 'high', 'ap', 'is', 'a', 'bit', 'overstated', '.', 'it', 'is', 'important', 'to', 'demonstrate', 'the', 'different', 'behavior', 'between', 'classification', 'and', 'detection', 'networks', ',', 'however', ',', 'the', 'same', 'improvement', 'is', 'observed', 'irrespective', 'of', 'the', 'teacher', 'ap', 'performance', '.', '-', 'the', 'structure', 'of', 'the', 'paper', 'can', 'be', 'improved', '.', 'the', 'related', 'work', 'section', 'for', 'example', 'can', 'be', 'moved', 'earlier', 'in', 'the', 'paper', 'to', 'give', 'the', 'bigger', 'picture', 'and', 'the', 'position', 'of', 'this', 'work', 'with', 'respect', 'to', 'the', 'literature', '.', '-', 'for', 'a', 'more', 'complete', 'comparison', 'yolo', 'and', 'ssd', 'networks', 'should', 'be', 'included', '.', '-', 'in', 'the', 'qualitative', 'analysis', 'observation', '(', 'ii', ')', 'states', 'that', 'the', 'proposed', 'methods', 'leads', 'to', 'single', 'box', 'per', 'object', 'compared', 'to', 'the', 'baselines', '.', 'in', 'this', 'case', 'do', 'the', 'baselines', 'use', 'non-maximum', 'suppression', '?', 'this', 'is', 'a', 'standard', 'post', 'processing', 'technique', 'that', 'alleviates', 'this', 'problem', '.', 'no', 'explanation', 'is', 'given', 'on', 'why', 'the', 'proposed', 'method', 'leads', 'to', 'this', 'behavior', '.', 'the', 'authors', 'have', 'made', 'clear', 'some', 'of', 'my', 'concerns', 'and', 'made', 'revisions', 'accordingly', '.', 'thus', 'i', 'am', 'in', 'a', 'position', 'now', 'to', 'recommend', 'this', 'paper', ',', 'thus', 'i', 'update', 'my', 'initial', 'recommendation', 'from', '5', 'to', '6', '.']",338,a8RiFt2IlY,1610040503192,1610474110215,1,uKhGRvM8QNH,Final Decision,Accept (Poster),"After the rebuttal stage, all reviewers lean positive (in final scores and/or in comments during the discussion phase). The AC found no reason to disagree. The benefit of the proposed method is demonstrated in many diverse settings, and the authors argue novelty in that no prior work addresses both fg/bg imbalance and relation distillation. ",ICLR2021,
